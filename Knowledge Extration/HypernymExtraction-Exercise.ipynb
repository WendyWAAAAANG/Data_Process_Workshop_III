{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypernym Relationship Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use NLTK and Hearst Pattern for hypernym relationship extraction. \n",
    "- Firstly, install python environment\n",
    "- Install NLTK: pip install nltk\n",
    "- Download data distribution for NLTK. Install using NLTK downloader: ``nltk.download()``. If cannot download using ``nltk.download()``, try download manually from https://github.com/nltk/nltk_data/tree/gh-pages![image.png](attachment:image.png) or https://pan.baidu.com/s/1wONWpaa86_wnsIksKda8eQ (code:tfon )\n",
    "- Unzip the downloaded file to the following folder: ``nltk.data.find(\".\")``\n",
    "- Unzip each zip file in the ten folders: *chunkers, corpora, grammers, help, misc, models, sentiment, stemmers, taggers, tokenizers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyponym Extraction using Hearst Pattern\n",
    "Hyponym extraction follows the following 4 steps:\n",
    "- Noun phrase chunking or named eneity chunking. You can use any np chunking/named entity technique.\n",
    "- Chunked sentences prepare. Traverse the chunked result, if the label is ``NP``, then merge all the words in this chunk and add a prefix ``NP_`` (for subsequence process).\n",
    "- Chunking refinement. If two or more NPs next to each other should be merged into a single NP. Eg., *\"NP_foo NP_bar blah blah\"* becomes *\"NP_foo_bar blah blah\"*\n",
    "- Find the hypernym and hyponym pairs based on the refined prepared chunked sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import pos_tag, word_tokenize, Tree\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression practice: In this example, we show one regex pattern example for Hearst pattern: ``NP such as {NP,}* {(or | and)} NP`` (https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_1 such as NP_2 , NP_3 and NP_4 \n"
     ]
    }
   ],
   "source": [
    "regex = r\"(NP_\\w+ (, )?such as (NP_\\w+ ?(, )?(and |or )?)+)\"\n",
    "test_str = \"NP_1 such as NP_2 , NP_3 and NP_4 \"\n",
    "matches = re.search(regex, test_str)\n",
    "if matches:\n",
    "    # Match.group([group1, ...]) Returns one or more subgroups of the match. \n",
    "    # If there is a single argument, the result is a single string;\n",
    "    # if there are multiple arguments, the result is a tuple with one item per argument. \n",
    "    # Without arguments, group1 defaults to zero (the whole match is returned).\n",
    "    print(matches.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Chunking Sentence\n",
    "- Note the result is not the chunked np, instead is the chunk tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  listen/VB\n",
      "  to/TO\n",
      "  music/NN\n",
      "  from/IN\n",
      "  musical/JJ\n",
      "  genres/NNS\n",
      "  ,/,\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  blues/NNS\n",
      "  ,/,\n",
      "  rock/NN\n",
      "  and/CC\n",
      "  jazz/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "def np_chunking(sentence):\n",
    "    # your implementation\n",
    "    result = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "    return result\n",
    "\n",
    "print(np_chunking(\"\"\"I like to listen to music from musical genres,such as blues,rock and jazz.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Prepare the chunked result for subsequent Hearst pattern matching\n",
    "- Traverse the chunked result, if the label is ``NP``, then merge all the words in this chunk and add a prefix ``NP_``\n",
    "- All the tokens are separated with a white space (``\" \"``) \n",
    "- Remember to lemmatize words, using ``WordNetLemmatizer`` (``from nltk.stem import WordNetLemmatizer``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the chunked sentence by merging words and add prefix NP_\n",
    "def prepare_chunks(chunks):\n",
    "    # If chunk is NP, start with NP_ and join tokens in chunk with _ ; Else just keep the token as it is\n",
    "    terms = []\n",
    "    # define regex expression of NP label.\n",
    "    grammar = \"NP: {<JJ>*<NN.*>+}\\n{<NN.*>+}\\n\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    chunks = cp.parse(chunks)\n",
    "    # chunks.draw()\n",
    "    for chunk in chunks:\n",
    "        label = None\n",
    "        try:\n",
    "            # see if the chunk is simply a word or a NP. But non-NP fail on this method call\n",
    "            label = chunk.label()\n",
    "        except:\n",
    "            pass\n",
    "        # Based on the label, do processing, your implementation here...\n",
    "        if type(chunk) == Tree and (str(label) == 'NP' or str(label) == 'GPE' or str(label) == 'PERSON'):\n",
    "            if chunk[0][0] == 'such' or chunk[0][0] == 'other':\n",
    "                terms.append(chunk[0][0])\n",
    "                np = \"NP_\" + \"_\".join([WordNetLemmatizer().lemmatize(a[0]) for a in chunk[1:]])\n",
    "                # np = \"NP_\" + \"_\".join(a[0] for a in chunk[1:])\n",
    "            else:\n",
    "                np = \"NP_\" + \"_\".join([WordNetLemmatizer().lemmatize(a[0]) for a in chunk])\n",
    "                # np = \"NP_\" + \"_\".join(a[0] for a in chunk)\n",
    "            terms.append(np)\n",
    "        else:\n",
    "            terms.append(chunk[0])\n",
    "    return ' '.join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... NP_work by such NP_author as NP_Herrick , NP_Goldsmith , and NP_Shakespeare .\n"
     ]
    }
   ],
   "source": [
    "# raw_text = \"I like to listen to music from musical genres,such as blues,rock and jazz.\"\n",
    "# raw_text = \"Agar is a substance prepared from a mixture of red algae, such as Gelidium,for laboratory or industrial use.\"\n",
    "raw_text = \"... works by such authors as Herrick, Goldsmith, and Shakespeare.\"\n",
    "chunk_res = np_chunking(raw_text)\n",
    "print(prepare_chunks(chunk_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Refinement chunking\n",
    "If two or more NPs next to each other should be merged into a single NP. E.g., ``NP_foo NP_bar blah blah`` becomes ``NP_foo_bar blah blah``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_NP(prepared_chunks):\n",
    "    sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\",lambda m: m.expand(r'\\1').replace(\" NP_\", \"_\"),prepared_chunks)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NP_foo_bar blah blah'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_NP(\"NP_foo NP_bar blah blah\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Find the hypernym and hyponyms on processed chunked results\n",
    "- Define Hearst patterns. Besides the regex, we also need to specify whether the hypernym is in the first part or the second part in the pattern.\n",
    "  - For example, in the pattern ``NP1 such as NP2 AND NP3``, the hypernym is the first part of the pattern; in the pattern ``NP1 , NP2 and other NP3``, the hypernym is the last part of the pattern. \n",
    "- After regex matching, find all the NPs and extract the hypernym and hyponym pairs based on the ``first`` or ``last`` attribute.\n",
    "- Clean the NPs by removing the prefix ``NP_`` and ``_``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NP_blue', 'NP_musical_genre'), ('NP_rock', 'NP_musical_genre'), ('NP_jazz', 'NP_musical_genre')]\n",
      "[('NP_basketball', 'NP_sport'), ('NP_football', 'NP_sport')]\n"
     ]
    }
   ],
   "source": [
    "# Given by the prepared text, return the hypernym-hyponym pairs\n",
    "def hyponym_extract(prepared_text, hearst_patterns):\n",
    "    # your implementation\n",
    "    pairs = []\n",
    "    for (pattern, parser) in hearst_patterns:\n",
    "        matches = re.search(pattern, prepared_text)\n",
    "        if matches:\n",
    "            match_str = matches.group(0)\n",
    "            # find all NP_xx and save to a list.\n",
    "            nps = [i for i in match_str.split(\" \") if i.startswith(\"NP_\")]\n",
    "            if parser == \"first\":\n",
    "                hypernym = nps[0]\n",
    "                hyponyms = nps[1:]\n",
    "            else:\n",
    "                hypernym = nps[-1]\n",
    "                hyponyms = nps[:-1]\n",
    "            for item in hyponyms:\n",
    "                pairs.append((item, hypernym))\n",
    "    return pairs\n",
    "\n",
    "hearst_patterns = [(r\"(NP_\\w+ (, )?such as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                    (r\"((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)\", \"last\"),\n",
    "                    (r\"(such NP_\\w+ (, )?as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                    (r\"(NP_\\w+ ?(, )?including (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                    (r\"(NP_\\w+ ?(, )?especially (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),]  # two examples for hearst pattern\n",
    "\n",
    "print(hyponym_extract(prepare_chunks(np_chunking(\"I like to listen to music from musical genres, such as blues,rock and jazz.\")), hearst_patterns))\n",
    "print(hyponym_extract(prepare_chunks(np_chunking(\"He likes to play basketball,football and other sports.\")), hearst_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NP_blue', 'NP_musical_genre'), ('NP_rock', 'NP_musical_genre'), ('NP_jazz', 'NP_musical_genre')]\n",
      "[('NP_basketball', 'NP_sport'), ('NP_football', 'NP_sport')]\n"
     ]
    }
   ],
   "source": [
    "def find_hyponyms(sentence, hearst_patterns):\n",
    "    # your implementation\n",
    "    res = hyponym_extract(prepare_chunks(np_chunking(sentence)), hearst_patterns)\n",
    "    return res\n",
    "\n",
    "print(find_hyponyms(\"\"\"I like to listen to music from musical genres,such as blues,rock and jazz.\"\"\", hearst_patterns))\n",
    "print(find_hyponyms(\"\"\"He likes to play basketball,football and other sports.\"\"\",hearst_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'football'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_np(term):\n",
    "    return term.replace(\"NP_\", \"\").replace(\"_\", \" \")\n",
    "clean_np('NP_football')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Program for Hypernym extraction using Hearst Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge everything to get the final extractor\n",
    "class HearstPatterns(object):\n",
    "    # finish the extractor class using the aforementioned functions\n",
    "    def __init__(self):\n",
    "        self.hearst_patterns = [(r\"(NP_\\w+ (, )?such as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                        (r\"((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)\", \"last\"),\n",
    "                        (r\"(such NP_\\w+ (, )?as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                        (r\"(NP_\\w+ ?(, )?including (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                        (r\"(NP_\\w+ ?(, )?especially (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),]\n",
    "\n",
    "    # Step1:\n",
    "    def np_chunking(self, sentence):\n",
    "        # your implementation\n",
    "        result = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "        return result\n",
    "\n",
    "    # Step2:\n",
    "    # prepare the chunked sentence by merging words and add prefix NP_\n",
    "    def prepare_chunks(self, chunks):\n",
    "        # If chunk is NP, start with NP_ and join tokens in chunk with _ ; Else just keep the token as it is\n",
    "        terms = []\n",
    "        # define regex expression of NP label.\n",
    "        grammar = \"NP: {<JJ>*<NN.*>+}\\n{<NN.*>+}\"\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        chunks = cp.parse(chunks)\n",
    "        for chunk in chunks:\n",
    "            label = None\n",
    "            try:\n",
    "                # see if the chunk is simply a word or a NP. But non-NP fail on this method call\n",
    "                label = chunk.label()\n",
    "            except:\n",
    "                pass\n",
    "            # Based on the label, do processing, your implementation here...\n",
    "            if type(chunk) == Tree and (str(label) == 'NP' or str(label) == 'GPE' or str(label) == 'PERSON'):\n",
    "                if chunk[0][0] == 'such' or chunk[0][0] == 'other':\n",
    "                    terms.append(chunk[0][0])\n",
    "                    # np = \"NP_\" + \"_\".join([WordNetLemmatizer().lemmatize(a[0]) for a in chunk[1:]])\n",
    "                    np = \"NP_\" + \"_\".join(a[0] for a in chunk[1:])\n",
    "                else:\n",
    "                    np = \"NP_\" + \"_\".join(a[0] for a in chunk)\n",
    "                terms.append(np)\n",
    "            else:\n",
    "                terms.append(chunk[0])\n",
    "            print('in perpare_chunks')\n",
    "            print(terms)\n",
    "        return ' '.join(terms)\n",
    "\n",
    "    def merge_NP(self, prepared_chunks):\n",
    "        sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\",lambda m: m.expand(r'\\1').replace(\" NP_\", \"_\"),prepared_chunks)\n",
    "        return sentence\n",
    "\n",
    "    # Step3: Find the hypernym and hyponyms on processed chunked results.\n",
    "    # Given by the prepared text, return the hypernym-hyponym pairs.\n",
    "    def hyponym_extract(self, prepared_text, hearst_patterns):\n",
    "        # your implementation\n",
    "        pairs = []\n",
    "        for (pattern, parser) in hearst_patterns:\n",
    "            matches = re.search(pattern, prepared_text)\n",
    "            if matches:\n",
    "                match_str = matches.group(0)\n",
    "                # find all NP_xx and save to a list.\n",
    "                nps = [i for i in match_str.split(\" \") if i.startswith(\"NP_\")]\n",
    "                if parser == \"first\":\n",
    "                    hypernym = nps[0]\n",
    "                    hyponyms = nps[1:]\n",
    "                else:\n",
    "                    hypernym = nps[-1]\n",
    "                    hyponyms = nps[:-1]\n",
    "                for item in hyponyms:\n",
    "                    pairs.append((item, hypernym))\n",
    "        return pairs\n",
    "    \n",
    "    def clean_np(self, term):\n",
    "        return term.replace(\"NP_\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    # function that call previous functions.\n",
    "    def find_hyponyms(self, sentence):\n",
    "        # your implementation\n",
    "        result = []\n",
    "        pre_chunks = merge_NP(prepare_chunks(np_chunking(sentence)))\n",
    "        pairs = hyponym_extract(pre_chunks, self.hearst_patterns)\n",
    "        for (hypo, hype) in pairs:\n",
    "            clean_hypo = clean_np(hypo)\n",
    "            clean_hype = clean_np(hype)\n",
    "            result.append((clean_hypo, clean_hype))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gelidium', 'red algae')]\n",
      "[('Herrick', 'author'), ('Goldsmith', 'author'), ('Shakespeare', 'author')]\n",
      "[('bistro', 'cheap eating place'), ('coffee shop', 'cheap eating place')]\n",
      "[('Canada', 'common law country'), ('England', 'common law country')]\n",
      "[('France', 'European country'), ('England', 'European country'), ('Spain', 'European country')]\n"
     ]
    }
   ],
   "source": [
    "# Test case for hearst patterns\n",
    "hp = HearstPatterns()\n",
    "\n",
    "test = [\"Agar is a substance prepared from a mixture of red algae, such as Gelidium,for laboratory or industrial use.\",\n",
    "                         \"... works by such authors as Herrick, Goldsmith, and Shakespeare.\",\n",
    "                         \"... bistros, coffee shops, and other cheap eating places.\",\n",
    "                         \"...all common law countries, including Canada and England.\",\n",
    "                         \"...most European countries, especially France, England, and Spain.\"]\n",
    "for txt in test:\n",
    "    hps = hp.find_hyponyms(txt)\n",
    "    print(hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "719d45e38b6b35dce474b00597395544d0fa9e2eb71949a83c153289e71e7b5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
