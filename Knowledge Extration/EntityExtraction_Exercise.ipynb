{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use NLTK for entity extraction. \n",
    "- Firstly, install python environment\n",
    "- Change the source mirror of pip : https://mirrors.bfsu.edu.cn/help/pypi/\n",
    "- Install NLTK: pip install nltk\n",
    "- Download data distribution for NLTK. Enter python terminal first. import nltk. Install packages by using NLTK downloader: ``nltk.download()``. If cannot download using ``nltk.download()``, try download manually from https://github.com/nltk/nltk_data/tree/gh-pages![image.png](attachment:image.png) or https://pan.baidu.com/s/1wONWpaa86_wnsIksKda8eQ (code:tfon )\n",
    "- Unzip the downloaded file to the following folder: ``nltk.data.find(\".\")``\n",
    "- Unzip each zip file in the ten folders: *chunkers, corpora, grammers, help, misc, models, sentiment, stemmers, taggers, tokenizers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages \n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'was',\n",
       " 'born',\n",
       " 'in',\n",
       " 'Liverpool',\n",
       " ',',\n",
       " 'to',\n",
       " 'Julia',\n",
       " 'Lennon',\n",
       " 'and',\n",
       " 'Alfred',\n",
       " 'Lennon']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentence:\n",
    "raw = \"\"\"John was born in Liverpool, to Julia Lennon and Alfred Lennon\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Liverpool', 'NNP'), (',', ','), ('to', 'TO'), ('Julia', 'NNP'), ('Lennon', 'NNP'), ('and', 'CC'), ('Alfred', 'NNP'), ('Lennon', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# pos-tag of inputs\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know the detail information of each tag, use the following statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking:\n",
    "- Use ``ne_chunk`` provided by NLTK. ``ne_chunk`` needs part-of-speech annotations to add ``NE`` labels to the sentence. The output of the ``ne_chunk`` is a ``nltk.Tree`` object\n",
    "- ``ne_chunk`` produces 2-level trees:\n",
    " - Nodes on Level-1: outsides any chunk\n",
    " - Nodes on Level-2: inside a chunk (the label of the chunk is denoted by the label of the subtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Liverpool/NNP)\n",
      "  ,/,\n",
      "  to/TO\n",
      "  (PERSON Julia/NNP Lennon/NNP)\n",
      "  and/CC\n",
      "  (PERSON Alfred/NNP Lennon/NNP))\n"
     ]
    }
   ],
   "source": [
    "chunks = ne_chunk(pos_tag(word_tokenize(raw)))\n",
    "print(chunks)\n",
    "chunks.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverse the chunked tree structure to get each chunk and words inside each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON John/NNP) <class 'nltk.tree.tree.Tree'>\n",
      "Chunk detect!\n",
      "John NNP\n",
      "('was', 'VBD') <class 'tuple'>\n",
      "('born', 'VBN') <class 'tuple'>\n",
      "('in', 'IN') <class 'tuple'>\n",
      "(GPE Liverpool/NNP) <class 'nltk.tree.tree.Tree'>\n",
      "Chunk detect!\n",
      "Liverpool NNP\n",
      "(',', ',') <class 'tuple'>\n",
      "('to', 'TO') <class 'tuple'>\n",
      "(PERSON Julia/NNP Lennon/NNP) <class 'nltk.tree.tree.Tree'>\n",
      "Chunk detect!\n",
      "Julia NNP\n",
      "Lennon NNP\n",
      "('and', 'CC') <class 'tuple'>\n",
      "(PERSON Alfred/NNP Lennon/NNP) <class 'nltk.tree.tree.Tree'>\n",
      "Chunk detect!\n",
      "Alfred NNP\n",
      "Lennon NNP\n"
     ]
    }
   ],
   "source": [
    "for i in chunks:\n",
    "    print(i, type(i))\n",
    "    if type(i) == Tree:\n",
    "        print('Chunk detect!')\n",
    "        chunk_phrase = []\n",
    "        for token,pos in i.leaves():\n",
    "            print(token,pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise1\n",
    "Extract all named entities as well as its type/label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'John': 'PERSON',\n",
       " 'Liverpool': 'GPE',\n",
       " 'Julia Lennon': 'PERSON',\n",
       " 'Alfred Lennon': 'PERSON'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise1, define a function to extract all named enties together with labels\n",
    "def get_labeled_chunks(text):\n",
    "    # your implementation\n",
    "    # chunk the sentense and assign a POS tag.\n",
    "    chunks = ne_chunk(pos_tag(word_tokenize(text)), binary = False)\n",
    "    label_entities = {}\n",
    "    # concat word entities with their tags.\n",
    "    for i in chunks:\n",
    "        if type(i) == Tree:\n",
    "            # concat noun phrases if it is.\n",
    "            # np = \" \".join([WordNetLemmatizer().lemmatize(a[0]) for a in i])\n",
    "            np = \" \".join(a[0] for a in i)\n",
    "            # append items to label_entities dictionary.\n",
    "            label_entities[np] = i.label()\n",
    "    return label_entities\n",
    "get_labeled_chunks(raw)\n",
    "\n",
    "#output result:\n",
    "#{'John': 'PERSON',\n",
    "# 'Liverpool': 'GPE',\n",
    "# 'Julia Kim': 'PERSON',\n",
    "# 'Alfred Lennon': 'PERSON'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise2\n",
    "Extract only *PERSON* entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'Julia Lennon', 'Alfred Lennon']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise2, extract all the entities of specific type\n",
    "def get_type_chunks(text, label):\n",
    "    # your implementation\n",
    "    # chunk the sentense and assign a POS tag.\n",
    "    chunks = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    entity = []\n",
    "    for i in chunks:\n",
    "        if type(i) == Tree and str(i.label()) == label:\n",
    "            # concat noun phrases if it is.\n",
    "            # np = \" \".join([WordNetLemmatizer().lemmatize(a[0]) for a in i])\n",
    "            np = \" \".join(a[0] for a in i)\n",
    "            # append items to entity list.\n",
    "            entity.append(np)\n",
    "    return entity\n",
    "get_type_chunks(raw,'PERSON')\n",
    "#output result:\n",
    "#['John', Julia Lennon', 'Alfred Lennon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise3: Noun phrase chunking\n",
    "Define your own grammer for noun phrase chunking using ``nltk.RegexpParser``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little dog', 'cat']\n",
      "['Jonh', 'Liverpool', 'Julia Kim', 'Alfred Lennon']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def np_chunking(sentence):\n",
    "    # define grammar to extract noun phrases.\n",
    "    grammer = \"NP: {<JJ>*<NN.*>+}\\n{<NN.*>+}\"  # chunker rule(s), try think of more rules\n",
    "    # your implementation\n",
    "    cp = nltk.RegexpParser(grammer)\n",
    "    # chunk the sentense and assign a POS tag as well as grammar.\n",
    "    chunks = cp.parse(pos_tag(word_tokenize(sentence)))\n",
    "    entity = []\n",
    "    for i in chunks:\n",
    "        if type(i) == Tree:\n",
    "            # concat noun phrases if it is.\n",
    "            # np = \" \".join([WordNetLemmatizer().lemmatize(a[0]) for a in i])\n",
    "            np = \" \".join(a[0] for a in i)\n",
    "            # append items to entity list.\n",
    "            entity.append(np)\n",
    "    return entity\n",
    "\n",
    "print(np_chunking(\"\"\"the little dog barked at the cat\"\"\"))\n",
    "print(np_chunking(\"\"\"Jonh was born in Liverpool, to Julia Kim and Alfred Lennon\"\"\"))\n",
    "\n",
    "#output result:\n",
    "#['little dog', 'cat']\n",
    "#['Jonh', 'Liverpool', 'Julia Kim', 'Alfred Lennon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Reference: https://www.cnblogs.com/chen8023miss/p/11458571.html."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "719d45e38b6b35dce474b00597395544d0fa9e2eb71949a83c153289e71e7b5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
